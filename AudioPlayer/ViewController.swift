// ViewController.swift
// AudioPlayer
// Created by Yusuke Abe, Tadija Ciric, Anthony Smaldore on 12/6/24.
//
// Main controller for the Clarity app. Handles:
// - Real-time audio transcription
// - Translation of transcribed text
// - Audio playback of the translation
// - Bluetooth media button integration
// - UI theming and state control

import UIKit
import AVFoundation
import Speech
import Translation
import MediaPlayer
import SwiftUI


// `ViewController` is responsible for managing user input via microphone,
//  performing transcription, translating text, and playing back translated audio.
class ViewController: UIViewController, SFSpeechRecognizerDelegate, AVAudioPlayerDelegate {
    
    // Audio & Speech Properties
    var audioPlayer: AVAudioPlayer?
    var isRecording = false
    var audioFileName: URL?
    
    // User-selected input and output language codes.
    var inputLanguage: String = "en-US"
    var outputLanguage: String = "es"
    var finalTranscriptionText: String = ""
    var finalTranslatedText: String = ""
    
    var speechRecognizer: SFSpeechRecognizer?
    var recognitionRequest: SFSpeechAudioBufferRecognitionRequest?
    var recognitionTask: SFSpeechRecognitionTask?
    let audioEngine = AVAudioEngine()
    var speechSynthesizer = AVSpeechSynthesizer()
    var speechSynthesizerDelegate: SpeechSynthesizerDelegate?
    var autoGeneratedAudioFile: URL?
    
    // Silent player maintains control over Bluetooth media controls.
    var silentPlayer: AVAudioPlayer?
    
    
    // UI Elements (Connected via Storyboard)
    @IBOutlet weak var recordButton: UIButton!
    @IBOutlet weak var stopRecordButton: UIButton!
    @IBOutlet weak var transcriptionTextView: UITextView!
    @IBOutlet weak var translationTextView: UITextView!
    @IBOutlet weak var playTranslatedAudioButton: UIButton!
    
    // Gradient layer property.
    var gradientLayer: CAGradientLayer?
    
    
    
    
    // Lifecycle Methods
    override func viewDidLoad() {
        super.viewDidLoad()
        applyGradientBackground()
        setupAudioSession()
        requestSpeechPermission()
        styleUI()
        
        // Set initial button states.
        recordButton.alpha = 1
        stopRecordButton.alpha = 0
        
        // Setup Bluetooth remote control integration
        let commandCenter = MPRemoteCommandCenter.shared()
        commandCenter.togglePlayPauseCommand.isEnabled = true
        commandCenter.togglePlayPauseCommand.addTarget { event in
            print("‚èØ MPRemoteCommandCenter togglePlayPause received")
            DispatchQueue.main.async {
                if self.audioEngine.isRunning {
                    self.handleStopRecording()
                } else {
                    self.handleRecord()
                }
            }
            return .success
        }
        
        commandCenter.playCommand.isEnabled = true
        commandCenter.playCommand.addTarget { event in
            print("‚ñ∂Ô∏è MPRemoteCommandCenter playCommand received")
                DispatchQueue.main.async {
                    if self.audioEngine.isRunning {
                        self.handleStopRecording()
                    } else {
                        self.handleRecord()
                    }
                }
                return .success
        }

        commandCenter.pauseCommand.isEnabled = true
        commandCenter.pauseCommand.addTarget { event in
            print("‚è∏ MPRemoteCommandCenter pauseCommand received")
            DispatchQueue.main.async { self.handleStopRecording() }
            return .success
        }
        print("‚úÖ Using Input Language: \(inputLanguage), Output Language: \(outputLanguage)")
    }
    
    
    
    override func viewDidLayoutSubviews() {
        super.viewDidLayoutSubviews()
        // Ensure the gradient layer covers the entire view.
        gradientLayer?.frame = view.bounds
    }
    
    
    
    override func viewDidAppear(_ animated: Bool) {
        super.viewDidAppear(animated)
        UIApplication.shared.beginReceivingRemoteControlEvents()
        self.becomeFirstResponder()
        
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.5) {
            print("üéØ Is first responder?", self.isFirstResponder ? "YES" : "NO")
        }
        startSilentPlayback()
        print("‚úÖ ViewController appeared successfully!")
    }
    
    
    
    
    override func viewWillDisappear(_ animated: Bool) {
        super.viewWillDisappear(animated)
        print("‚ö†Ô∏è ViewController is disappearing, cleaning up translation tasks.")
        MPRemoteCommandCenter.shared().togglePlayPauseCommand.removeTarget(nil)
        UIApplication.shared.endReceivingRemoteControlEvents()
        self.resignFirstResponder()
        silentPlayer?.stop()
        print("üõë Silent playback stopped (view disappeared).")
        
        // Remove any embedded SwiftUI TranslationView.
        for child in children {
            if let hostingController = child as? UIHostingController<TranslationView> {
                hostingController.willMove(toParent: nil)
                hostingController.view.removeFromSuperview()
                hostingController.removeFromParent()
            }
        }
        self.finalTranslatedText = ""
    }
    
    
    
    override var canBecomeFirstResponder: Bool {
        return true
    }
    
    
    
    // Configures the gradient background for aesthetic UI.
    func applyGradientBackground() {
        let gradient = CAGradientLayer()
        gradient.colors = [
            UIColor(hex: "#23252c").cgColor,
            UIColor(hex: "#40607e").cgColor,
            UIColor(hex: "#584d78").cgColor
        ]
        gradient.startPoint = CGPoint(x: 0, y: 0)
        gradient.endPoint = CGPoint(x: 1, y: 1)
        gradient.frame = view.bounds
        view.layer.insertSublayer(gradient, at: 0)
        gradientLayer = gradient
    }
    
    
    
    // Applies consistent UI styling across all visible elements.
    func styleUI() {
        // Set a modern, clean font for buttons.
        let buttonFont = UIFont.systemFont(ofSize: 16, weight: .medium)
        recordButton.titleLabel?.font = buttonFont
        stopRecordButton.titleLabel?.font = buttonFont
        playTranslatedAudioButton.titleLabel?.font = buttonFont
        
        // Style record button.
        recordButton.backgroundColor = UIColor(hex: "#40607e")
        recordButton.setTitleColor(.white, for: .normal)
        recordButton.layer.cornerRadius = 8
        
        // Style stop record button.
        stopRecordButton.backgroundColor = UIColor(hex: "#40607e")
        stopRecordButton.setTitleColor(.white, for: .normal)
        stopRecordButton.layer.cornerRadius = 8
        
        // Style play translated audio button.
        playTranslatedAudioButton.backgroundColor = UIColor(hex: "#40607e")
        playTranslatedAudioButton.setTitleColor(.white, for: .normal)
        playTranslatedAudioButton.layer.cornerRadius = 8
        
        // Style transcription text view.
        transcriptionTextView.backgroundColor = UIColor(hex: "#584d78")
        transcriptionTextView.textColor = .white
        transcriptionTextView.font = UIFont.systemFont(ofSize: 16, weight: .regular)
        transcriptionTextView.layer.cornerRadius = 8
        transcriptionTextView.clipsToBounds = true
        transcriptionTextView.isEditable = false
        transcriptionTextView.isSelectable = false
        
        // Style translation text view.
        translationTextView.backgroundColor = UIColor(hex: "#584d78")
        translationTextView.textColor = .white
        translationTextView.font = UIFont.systemFont(ofSize: 16, weight: .regular)
        translationTextView.layer.cornerRadius = 8
        translationTextView.clipsToBounds = true
        translationTextView.isEditable = false
        translationTextView.isSelectable = false
    }
    
    
    
    // Requests permission to access speech recognition.
    func requestSpeechPermission() {
        SFSpeechRecognizer.requestAuthorization { status in
            DispatchQueue.main.async {
                switch status {
                case .authorized:
                    print("‚úÖ Speech recognition authorized.")
                    self.recordButton?.isEnabled = true
                case .denied:
                    print("‚ùå User denied speech recognition.")
                    self.recordButton?.isEnabled = false
                case .restricted, .notDetermined:
                    print("‚ùå Speech recognition not available.")
                    self.recordButton?.isEnabled = false
                @unknown default:
                    fatalError()
                }
            }
        }
    }
    
    
    
    // Sets up the audio session for recording and playback with Bluetooth support.
    func setupAudioSession() {
        let session = AVAudioSession.sharedInstance()
        do {
            try session.setCategory(.playAndRecord, mode: .moviePlayback, options: [.allowBluetooth, .allowBluetoothA2DP, .mixWithOthers, .duckOthers])
            if let inputs = AVAudioSession.sharedInstance().availableInputs {
                print("üéô Inputs AFTER setting .playAndRecord mode:")
                for input in inputs {
                    print(" - \(input.portType.rawValue): \(input.portName)")
                }
            }

            try session.setActive(true)
            let isOtherAudioPlaying = AVAudioSession.sharedInstance().isOtherAudioPlaying
            print("üéö AVAudioSession active. Is other audio playing? \(isOtherAudioPlaying ? "YES" : "NO")")

        } catch {
            print("‚ùå Failed to setup audio session: \(error)")
        }
    }
    
    
    
    // Begins silent audio playback to maintain control over Bluetooth accessories.
    func startSilentPlayback() {
        guard let silenceURL = Bundle.main.url(forResource: "silence", withExtension: "mp3") else {
            print("‚ùå Missing silence.mp3 in bundle.")
            return
        }
        
        do {
            silentPlayer = try AVAudioPlayer(contentsOf: silenceURL)
            silentPlayer?.numberOfLoops = -1
            silentPlayer?.volume = 0.01
            silentPlayer?.prepareToPlay()
            silentPlayer?.play()
            if let player = silentPlayer {
                print("üéß Silent player state:")
                print(" - isPlaying:", player.isPlaying)
                print(" - duration:", player.duration)
                print(" - volume:", player.volume)
                print(" - currentTime:", player.currentTime)
            } else {
                print("‚ùå silentPlayer is nil after attempt to play.")
            }
            print("üîà Started silent audio playback to claim media button control.")
        } catch {
            print("‚ùå Could not start silent audio playback:", error)
        }
    }



    // Forces AVAudioSession to prefer a Bluetooth microphone if one is available.
    func forceBluetoothInputIfAvailable() {
        let audioSession = AVAudioSession.sharedInstance()
        
        guard let inputs = audioSession.availableInputs else {
            print("‚ùå No available audio inputs found.")
            return
        }

        print("üîç Checking available audio inputs:")
        for input in inputs {
            print(" - \(input.portType.rawValue): \(input.portName)")
        }

        if let bluetoothInput = inputs.first(where: {
            $0.portType == .bluetoothHFP || $0.portType == .bluetoothLE
        }) {
            do {
                try audioSession.setPreferredInput(bluetoothInput)
                print("‚úÖ Forced preferred input to Bluetooth: \(bluetoothInput.portName)")
            } catch {
                print("‚ùå Failed to set preferred Bluetooth input:", error)
            }
        } else {
            print("‚ö†Ô∏è No Bluetooth input available to select.")
        }
    }
    
    
    
    // Begins real-time transcription using selected input language.
    func startRealTimeTranscription() {
        speechRecognizer = SFSpeechRecognizer(locale: Locale(identifier: inputLanguage))
        guard let speechRecognizer = speechRecognizer else {
            print("‚ùå Speech recognizer not available for language: \(inputLanguage)")
            return
        }
        
        forceBluetoothInputIfAvailable()
        
        recognitionRequest = SFSpeechAudioBufferRecognitionRequest()
        recognitionRequest?.shouldReportPartialResults = true
        
        let inputNode = audioEngine.inputNode
        inputNode.removeTap(onBus: 0)
        
        inputNode.installTap(onBus: 0, bufferSize: 1024, format: inputNode.outputFormat(forBus: 0)) { buffer, when in
            self.recognitionRequest?.append(buffer)
        }
        
        audioEngine.prepare()
        do {
            try audioEngine.start()
            print("üé§ Audio engine started for transcription.")
        } catch {
            print("‚ùå Failed to start audio engine:", error)
        }
        
        recognitionTask = speechRecognizer.recognitionTask(with: recognitionRequest!) { result, error in
            if let result = result {
                let transcribedText = result.bestTranscription.formattedString
                print("üìù Transcribed: \(transcribedText)")
                
                if self.recognitionTask == nil {
                    print("‚ö†Ô∏è Skipping translation: Recognition was canceled.")
                    return
                }
                
                DispatchQueue.main.async {
                    self.transcriptionTextView.text = transcribedText
                    self.presentTranslationView(with: transcribedText)
                }
            }
            
            if let error = error as NSError? {
                if error.code == 1110 {
                    print("‚ö†Ô∏è Ignoring 'No speech detected' error (User stopped speaking).")
                    return
                } else if error.code == 301 {
                    print("‚ö†Ô∏è Ignoring 'Recognition request was canceled' error (User stopped recording).")
                    return
                }
                print("‚ùå Transcription error: \(error.localizedDescription) \(error.code)")
            }
        }
    }
    
    
    
    // Embeds the SwiftUI TranslationView for async translation of the provided text.
    func presentTranslationView(with text: String) {
        guard self.isViewLoaded && self.view.window != nil else {
            print("‚ö†Ô∏è Skipping translation: ViewController is disappearing.")
            return
        }
        
        let swiftUIView = TranslationView(
            textToTranslate: text,
            sourceLanguage: Locale.Language(identifier: inputLanguage),
            targetLanguage: Locale.Language(identifier: outputLanguage)
        ) { translatedText in
            DispatchQueue.main.async {
                print("‚úÖ Translated: \(translatedText)")
                self.finalTranscriptionText = text
                self.finalTranslatedText = translatedText
                self.transcriptionTextView.text = text
                self.translationTextView.text = translatedText
            }
        }
        
        let hostingController = UIHostingController(rootView: swiftUIView)
        addChild(hostingController)
        // Hide the hosting controller completely.
        hostingController.view.frame = CGRect(x: 0, y: 0, width: 0, height: 0)
        hostingController.view.isHidden = true
        view.addSubview(hostingController.view)
        hostingController.didMove(toParent: self)
    }
    
    
    // Synthesizes translated text into spoken audio.
    func generateAudioFromTranslation(_ text: String) {
        print("üîä Generating audio from translated text (FINAL).")
        
        let speechUtterance = AVSpeechUtterance(string: text)
        speechUtterance.voice = AVSpeechSynthesisVoice(language: outputLanguage)
        
        let timestamp = Int(Date().timeIntervalSince1970 * 1000)
        let fileURL = getDocumentsDirectory().appendingPathComponent("translatedSpeech_\(timestamp).m4a")
        
        self.autoGeneratedAudioFile = fileURL       //rename
        print("‚úÖ Saving translated speech to: \(fileURL)")
        
        speechSynthesizerDelegate = SpeechSynthesizerDelegate(completion: {
            DispatchQueue.main.async {
                print("‚úÖ Finished Playing Translated Audio")
            }
        })
        
        do {
            try AVAudioSession.sharedInstance().overrideOutputAudioPort(.none)
            print("üîÑ Output audio port override set to .none (default routing).")
        } catch {
            print("‚ùå Failed to override output audio port: \(error)")
        }
        
        let route = AVAudioSession.sharedInstance().currentRoute
        print("üéß Audio route before playback:")
        for output in route.outputs {
            print(" - Port Type: \(output.portType.rawValue), Name: \(output.portName)")
        }
        
        speechSynthesizer.delegate = speechSynthesizerDelegate
        AudioUtils.switchAudioMode(toPlayback: true)
        print("üîç Available inputs:")
        
        for input in AVAudioSession.sharedInstance().availableInputs ?? [] {
            print(" - \(input.portType.rawValue): \(input.portName)")
        }

        speechSynthesizer.speak(speechUtterance)
        DispatchQueue.main.async {
            self.playTranslatedAudioButton?.isEnabled = true
        }
    }



    // Retrieves the app's documents directory URL.
    func getDocumentsDirectory() -> URL {
        let paths = FileManager.default.urls(for: .documentDirectory, in: .userDomainMask)
        return paths[0]
    }

    
    
    // Handles start of transcription session.
    @IBAction func handleRecord() {
        if !audioEngine.isRunning {
            startRealTimeTranscription()
            UIView.animate(withDuration: 0.3) {
                self.recordButton.alpha = 0
                self.stopRecordButton.alpha = 1
            }
        }
    }
    
    
    
    // Handles stop of transcription session and begins translation.
    @IBAction func handleStopRecording() {
        print("üõë Stopping Recording...")
        
        // Detach the speech recognizer.
        self.recognitionTask?.cancel()
        self.recognitionTask = nil
        self.recognitionRequest = nil
        
        self.audioEngine.inputNode.removeTap(onBus: 0)
        self.audioEngine.stop()
        self.speechRecognizer = nil
        
        let finalText = self.transcriptionTextView.text?.trimmingCharacters(in: .whitespacesAndNewlines) ?? ""
        if finalText.isEmpty {
            print("‚ö†Ô∏è Ignoring empty final transcription. Nothing to translate.")
            return
        }
        
        print("üåç Finalizing translation for: \(finalText)")
        self.finalTranscriptionText = finalText
        self.presentTranslationView(with: finalText)
        
        DispatchQueue.main.asyncAfter(deadline: .now() + 1.5) {
            if !self.finalTranslatedText.isEmpty {
                self.generateAudioFromTranslation(self.finalTranslatedText)
            } else {
                print("‚ö†Ô∏è No translated text available for audio generation.")
            }
        }
        
        UIView.animate(withDuration: 0.3) {
            self.recordButton.alpha = 1
            self.stopRecordButton.alpha = 0
        }
    }
    
    
    
    // Plays the most recent translated audio clip.         (Supposed to be for Eleven Labs generated audio)
    @IBAction func handlePlayTranslatedAudio() {
        guard let audioFileURL = autoGeneratedAudioFile else {
            print("‚ö†Ô∏è No translated audio file available to play.")
            return
        }
        
        guard FileManager.default.fileExists(atPath: audioFileURL.path) else {
            print("‚ùå Translated audio file is missing: \(audioFileURL.path)")
            return
        }

        do {
            setupAudioSession()
            audioPlayer = try AVAudioPlayer(contentsOf: audioFileURL)
            audioPlayer?.prepareToPlay()
            
            let currentRoute = AVAudioSession.sharedInstance().currentRoute
            print("üéß Current audio output route:")
            for output in currentRoute.outputs {
                print(" - Port Type: \(output.portType.rawValue), Name: \(output.portName)")
            }
            
            do {
                try AVAudioSession.sharedInstance().overrideOutputAudioPort(.none)
                print("üîÑ Output audio port override set to .none (default routing).")
            } catch {
                print("‚ùå Failed to override output audio port: \(error)")
            }

            audioPlayer?.play()
            print("‚úÖ Playing translated audio.")
        } catch {
            print("‚ùå Failed to play translated audio:", error)
        }
    }
}



// Switches between .playback and .playAndRecord audio modes depending on use case.
class AudioUtils {
    static func switchAudioMode(toPlayback: Bool) {
        let audioSession = AVAudioSession.sharedInstance()
        do {
            if toPlayback {
                try audioSession.setCategory(.playback, mode: .default, options: [.allowBluetoothA2DP])
                print("üîä Switched to .playback mode for audio output.")
            } else {
                try audioSession.setCategory(.playAndRecord, mode: .moviePlayback, options: [.allowBluetooth, .allowBluetoothA2DP, .duckOthers, .mixWithOthers])
                print("üé§ Switched back to .playAndRecord mode for transcription + control.")
            }
            try audioSession.setActive(true)
        } catch {
            print("‚ùå Failed to switch audio mode:", error)
        }
    }
}




// Delegate to handle speech synthesizer completion.
class SpeechSynthesizerDelegate: NSObject, AVSpeechSynthesizerDelegate {
    private let completion: () -> Void
    init(completion: @escaping () -> Void) { self.completion = completion }
    
    func speechSynthesizer(_ synthesizer: AVSpeechSynthesizer, didFinish utterance: AVSpeechUtterance) {
        DispatchQueue.main.asyncAfter(deadline: .now() + 0.4) {
            AudioUtils.switchAudioMode(toPlayback: false)
        }
        completion()
        print("‚úÖ Finished speaking translated audio.")
    }
}


